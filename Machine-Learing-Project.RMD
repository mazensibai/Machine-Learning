---
title: "Machine_Learing"
output: html_document
---
#Introduction 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
The goal of your project is to predict the manner in which they did the exercise.

#Load necessary libraries
```{r}
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)
```

#Load the data
the loading process supposes that the files are in the Working dirctory 

###Import the data treating empty values as NA.
```{r}
data.df  <- read.csv("pml-training.csv", na.strings=c("NA","","#DIV/0!"), header=TRUE)
dataColumnName <- names(data.df)
test.df  <- read.csv("pml-testing.csv", na.strings=c("NA","","#DIV/0!"), header=TRUE)
testColumnName <- names(test.df)
summary(data.df $classe)
```
As we can see, there are five levels for the Classe column .

#Clean the Data 
here we will drop all the columns which has a high percentage of NA 
```{r}
na_test = sapply(data.df, function(x) {sum(is.na(x))})
table(na_test)
bad_columns = names(na_test[na_test>=19216])
data.df = data.df[, !names(data.df) %in% bad_columns]
test.df = test.df[, !names(data.df) %in% bad_columns]
```
Then we will drop all the non related columns (like time columns for example)
```{r}
data.df = data.df[,-c(1:7)]
test.df = test.df[,-c(1:7)]
```


#split the data
As the number of rows is huge,memory and CPU problems are expected when the system will apply the ML alogorithm on them, so better to split the data into smaller groups .

### Divide the given training set into 4 roughly equal sets.
```{r}
set.seed(666)
ids_small <- createDataPartition(y=data.df$classe, p=0.25, list=FALSE)
df_small1 <- data.df[ids_small,]
df_remainder <- data.df[-ids_small,]
set.seed(666)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.33, list=FALSE)
df_small2 <- df_remainder[ids_small,]
df_remainder <- df_remainder[-ids_small,]
set.seed(666)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.5, list=FALSE)
df_small3 <- df_remainder[ids_small,]
df_small4 <- df_remainder[-ids_small,]
```

### Divide each of these 4 sets into training (60%) and test (40%) sets.
```{r}
set.seed(666)
inTrain <- createDataPartition(y=df_small1$classe, p=0.6, list=FALSE)
df_small_training1 <- df_small1[inTrain,]
df_small_testing1 <- df_small1[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small2$classe, p=0.6, list=FALSE)
df_small_training2 <- df_small2[inTrain,]
df_small_testing2 <- df_small2[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small3$classe, p=0.6, list=FALSE)
df_small_training3 <- df_small3[inTrain,]
df_small_testing3 <- df_small3[-inTrain,]
set.seed(666)
inTrain <- createDataPartition(y=df_small4$classe, p=0.6, list=FALSE)
df_small_training4 <- df_small4[inTrain,]
df_small_testing4 <- df_small4[-inTrain,]
```

#Model Builinding
For classification we will use two algorithm: classification tree and random forest .

##Classification Tree
we will apply the algorithm on training dataset 1 without any preprocessing or cross validation
```{r}
set.seed(666)
modFit <- train(df_small_training1$classe ~ ., data = df_small_training1, method="rpart")
print(modFit, digits=3)
```
```{r}
print(modFit$finalModel, digits=3)
fancyRpartPlot(modFit$finalModel)
```

### Run against testing data set 1 
```{r}
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)
```
low accuracy rate (0.5584) , maybe we will get a better one by applying preprocess and cross validation


### Train on training set 1 of 4 with only preprocessing.
```{r}
set.seed(666)
modFit <- train(df_small_training1$classe ~ .,  preProcess=c("center", "scale"), data = df_small_training1, method="rpart")
print(modFit, digits=3)
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)
```
the same  low accuracy , trying with cross validation

### Train on training set 1 of 4 with only cross validation
```{r}
set.seed(666)
modFit <- train(df_small_training1$classe ~ .,  trControl=trainControl(method = "cv", number = 4), data = df_small_training1, method="rpart")
print(modFit, digits=3)
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)
```
not really improving, trying now both preprocessing and cross validation


### Train on training set 1 of 4 with both preprocessing and cross validation.
```{r}
set.seed(666)
modFit <- train(df_small_training1$classe ~ .,  preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data = df_small_training1, method="rpart")
print(modFit, digits=3)
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)
```
minmal improvment, still not enough, we can not depend on it.

##Random forest

###Train on training set 1 of 4 with only cross validation.
```{r}
set.seed(666)
modFit <- train(df_small_training1$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)
```

### Run against testing set 1 of 4.
```{r}
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)
```

###Run against 20 testing set .
```{r}
print(predict(modFit, newdata=test.df))
```


###Train on training set 1 of 4 with only both preprocessing and cross validation.
```{r}
set.seed(666)
modFit <- train(df_small_training1$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)
```

### Run against testing set 1 of 4.

```{r}
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)
```

###Run against 20 testing set .
```{r}
print(predict(modFit, newdata=test.df))
```

the accuracy rate droped from 0.9714 to 0.9709 with the addition of preprocessing. Thus I decided to apply only cross validation to the remaining 3 data sets.



###Train on training set 2 of 4 with only cross validation.
```{r}
set.seed(666)
modFit <- train(df_small_training2$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training2)
print(modFit, digits=3)
```
###Run against 20 testing set .
```{r}
print(predict(modFit, newdata=test.df))
```

###Run against testing set 2 of 4.
```{r}
predictions <- predict(modFit, newdata=df_small_testing2)
print(confusionMatrix(predictions, df_small_testing2$classe), digits=4)
```

###Train on training set 3 of 4 with only cross validation.
```{r}
set.seed(666)
modFit <- train(df_small_training3$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=df_small_training3)
print(modFit, digits=3)
```
###Run against testing set 3 of 4.
```{r}
predictions <- predict(modFit, newdata=df_small_testing3)
print(confusionMatrix(predictions, df_small_testing3$classe), digits=4)
```
###Run against 20 testing set .
```{r}
print(predict(modFit, newdata=test.df))
```

###Train on training set 4 of 4 with only cross validation.
```{r}
set.seed(666)
modFit <- train(df_small_training4$classe ~ ., method="rf",  trControl=trainControl(method = "cv", number = 4), data=df_small_training4)
print(modFit, digits=3)
```
###Run against testing set 4 of 4.
```{r}
predictions <- predict(modFit, newdata=df_small_testing4)
print(confusionMatrix(predictions, df_small_testing4$classe), digits=4)
```
###Run against 20 testing set .
```{r}
print(predict(modFit, newdata=test.df))
```

Out of Sample Error

According to Professor Leek's Week 1 “In and out of sample errors”, the out of sample error is the “error rate you get on new data set.” In my case, it's the error rate after running the predict() function on the 4 testing sets:

Random Forest (preprocessing and cross validation) Testing Set 1: 1 - .9714 = 0.0286

Random Forest (preprocessing and cross validation) Testing Set 2: 1 - 0.9619 = 0.0381

Random Forest (preprocessing and cross validation) Testing Set 3: 1 - .9655 = 0.0345

Random Forest (preprocessing and cross validation) Testing Set 4: 1 - 0.9558 = 0.0442


#Conclusion
Since options A and C above have the best outcome I will use them to submit the answers for the Projects Problem .

